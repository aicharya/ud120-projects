{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vectorize_text.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicharya/ud120-projects/blob/master/text_learning/vectorize_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU_j_J_RTnsX",
        "colab_type": "code",
        "outputId": "c515d83b-ed1d-4297-d84b-3cd173f52707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/aicharya/ud120-projects.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ud120-projects'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 5321 (delta 86), reused 0 (delta 0), pack-reused 5185\u001b[K\n",
            "Receiving objects: 100% (5321/5321), 19.94 MiB | 23.55 MiB/s, done.\n",
            "Resolving deltas: 100% (4537/4537), done.\n",
            "Checking out files: 100% (4726/4726), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ8EbpGbXtpV",
        "colab_type": "code",
        "outputId": "a5764785-45de-4b8d-9b47-5aaeee075f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import os\n",
        "os.chdir('ud120-projects/text_learning')\n",
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from_sara.txt',\n",
              " 'from_chris.txt',\n",
              " 'test_email.txt',\n",
              " 'vectorize_text.ipynb',\n",
              " 'text_steps.ipynb',\n",
              " 'vectorize_text.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amoIXGpJZF6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import sys\n",
        "\n",
        "sys.path.append( \"../tools/\" )\n",
        "from parse_out_email_text import parseOutText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4D-SIpZMRz",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "    Starter code to process the emails from Sara and Chris to extract\n",
        "    the features and get the documents ready for classification.\n",
        "    \n",
        "    The list of all the emails from Sara are in the from_sara list\n",
        "    likewise for emails from Chris (from_chris)\n",
        "    \n",
        "    The actual documents are in the Enron email dataset, which\n",
        "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
        "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
        "    The data is stored in lists and packed away in pickle files at the end.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc_9BzQpZOPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from_sara  = open(\"from_sara.txt\", \"r\")\n",
        "from_chris = open(\"from_chris.txt\", \"r\")\n",
        "\n",
        "from_data = []\n",
        "word_data = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY8R1m2KZhOY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "### temp_counter is a way to speed up the development--there are\n",
        "### thousands of emails from Sara and Chris, so running over all of them\n",
        "### can take a long time\n",
        "### temp_counter helps you only look at the first 200 emails in the list so you\n",
        "### can iterate your modifications quicker\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxV_oPqqZmFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_counter = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrG8jM7NZs0T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqEtbf1jZsLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "58ec4347-49e4-426d-eb55-949f94abdede"
      },
      "source": [
        "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
        "    for path in from_person:\n",
        "        ### only look at first 200 emails when developing\n",
        "        ### once everything is working, remove this line to run over full dataset\n",
        "        temp_counter += 1\n",
        "        if temp_counter < 200:\n",
        "            path = os.path.join('..', path[:-1])\n",
        "            print path\n",
        "            email = open(path, \"r\")\n",
        "\n",
        "            ### use parseOutText to extract the text from the opened email\n",
        "\n",
        "            ### use str.replace() to remove any instances of the words\n",
        "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
        "\n",
        "            ### append the text to word_data\n",
        "\n",
        "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
        "\n",
        "\n",
        "            email.close()\n",
        "            "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "../maildir/bailey-s/deleted_items/101.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IOError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8b5aa720849a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0memail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m### use parseOutText to extract the text from the opened email\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../maildir/bailey-s/deleted_items/101.'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCLJ2R5PaFKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print \"emails processed\"\n",
        "from_sara.close()\n",
        "from_chris.close()\n",
        "\n",
        "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
        "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHgZtBmmaJFT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "### in Part 4, do TfIdf vectorization here\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayPbC4f1aKZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}