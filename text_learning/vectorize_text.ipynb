{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vectorize_text.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicharya/ud120-projects/blob/master/text_learning/vectorize_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU_j_J_RTnsX",
        "colab_type": "code",
        "outputId": "c515d83b-ed1d-4297-d84b-3cd173f52707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!git clone https://github.com/aicharya/ud120-projects.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ud120-projects'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 5321 (delta 86), reused 0 (delta 0), pack-reused 5185\u001b[K\n",
            "Receiving objects: 100% (5321/5321), 19.94 MiB | 23.55 MiB/s, done.\n",
            "Resolving deltas: 100% (4537/4537), done.\n",
            "Checking out files: 100% (4726/4726), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ8EbpGbXtpV",
        "colab_type": "code",
        "outputId": "a5764785-45de-4b8d-9b47-5aaeee075f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import os\n",
        "os.chdir('ud120-projects/text_learning')\n",
        "os.listdir(os.getcwd())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from_sara.txt',\n",
              " 'from_chris.txt',\n",
              " 'test_email.txt',\n",
              " 'vectorize_text.ipynb',\n",
              " 'text_steps.ipynb',\n",
              " 'vectorize_text.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFaqh6UDaoaL",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "print \"downloading the Enron dataset (this may take a while)\"\n",
        "print \"to check on progress, you can cd up one level, then execute <ls -lthr>\"\n",
        "print \"Enron dataset should be last item on the list, along with its current size\"\n",
        "print \"download will complete at about 423 MB\"\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYGWSeGWalRE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "aead528b-aa11-480e-c507-6bdb59d002e1"
      },
      "source": [
        "import urllib\n",
        "url = \"https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz\"\n",
        "urllib.urlretrieve(url, filename=\"../enron_mail_20150507.tar.gz\") \n",
        "print \"download complete!\"\n",
        "\n",
        "\n",
        "print\n",
        "print \"unzipping Enron dataset (this may take a while)\"\n",
        "import tarfile\n",
        "import os\n",
        "os.chdir(\"..\")\n",
        "tfile = tarfile.open(\"enron_mail_20150507.tar.gz\", \"r:gz\")\n",
        "tfile.extractall(\".\")\n",
        "\n",
        "print \"you're ready to go!\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "download complete!\n",
            "\n",
            "unzipping Enron dataset (this may take a while)\n",
            "you're ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1cnXXuNpGIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d757d5b-1936-42f5-b34e-905b1664fda9"
      },
      "source": [
        "os.chdir(\"text_learning\")\n",
        "os.listdir(\"../maildir\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sanders-r',\n",
              " 'kuykendall-t',\n",
              " 'blair-l',\n",
              " 'salisbury-h',\n",
              " 'solberg-g',\n",
              " 'cash-m',\n",
              " 'hodge-j',\n",
              " 'meyers-a',\n",
              " 'zufferli-j',\n",
              " 'maggi-m',\n",
              " 'beck-s',\n",
              " 'storey-g',\n",
              " 'stepenovitch-j',\n",
              " 'ruscitti-k',\n",
              " 'bass-e',\n",
              " 'sager-e',\n",
              " 'shackleton-s',\n",
              " 'steffes-j',\n",
              " 'taylor-m',\n",
              " 'mclaughlin-e',\n",
              " 'allen-p',\n",
              " 'ward-k',\n",
              " 'hendrickson-s',\n",
              " 'linder-e',\n",
              " 'donoho-l',\n",
              " 'kaminski-v',\n",
              " 'lokey-t',\n",
              " 'thomas-p',\n",
              " 'weldon-c',\n",
              " 'davis-d',\n",
              " 'holst-k',\n",
              " 'gang-l',\n",
              " 'quenet-j',\n",
              " 'saibi-e',\n",
              " 'benson-r',\n",
              " 'mckay-b',\n",
              " 'quigley-d',\n",
              " 'south-s',\n",
              " 'hernandez-j',\n",
              " 'king-j',\n",
              " 'heard-m',\n",
              " 'delainey-d',\n",
              " 'tholt-j',\n",
              " 'pereira-s',\n",
              " 'ybarbo-p',\n",
              " 'hyatt-k',\n",
              " 'lucci-p',\n",
              " 'williams-w3',\n",
              " 'mims-thurston-p',\n",
              " 'slinger-r',\n",
              " 'horton-s',\n",
              " 'perlingiere-d',\n",
              " 'gilbertsmith-d',\n",
              " 'love-p',\n",
              " 'badeer-r',\n",
              " 'sturm-f',\n",
              " 'watson-k',\n",
              " 'white-s',\n",
              " 'crandell-s',\n",
              " 'fossum-d',\n",
              " 'lokay-m',\n",
              " 'shively-h',\n",
              " 'lenhart-m',\n",
              " 'lay-k',\n",
              " 'fischer-m',\n",
              " 'dickson-s',\n",
              " 'mcconnell-m',\n",
              " 'may-l',\n",
              " 'germany-c',\n",
              " 'kitchen-l',\n",
              " 'gay-r',\n",
              " 'ring-a',\n",
              " 'buy-r',\n",
              " 'arnold-j',\n",
              " 'skilling-j',\n",
              " 'whalley-l',\n",
              " 'townsend-j',\n",
              " 'panus-s',\n",
              " 'ermis-f',\n",
              " 'kean-s',\n",
              " 'rapp-b',\n",
              " 'staab-t',\n",
              " 'stclair-c',\n",
              " 'neal-s',\n",
              " 'causholli-m',\n",
              " 'pimenov-v',\n",
              " 'merriss-s',\n",
              " 'mckay-j',\n",
              " 'arora-h',\n",
              " 'lavorato-j',\n",
              " 'nemec-g',\n",
              " 'brawner-s',\n",
              " 'shapiro-r',\n",
              " 'corman-s',\n",
              " 'motley-m',\n",
              " 'parks-j',\n",
              " 'zipper-a',\n",
              " 'donohoe-t',\n",
              " 'hyvl-d',\n",
              " 'carson-m',\n",
              " 'smith-m',\n",
              " 'richey-c',\n",
              " 'geaccone-t',\n",
              " 'rogers-b',\n",
              " 'campbell-l',\n",
              " 'semperger-c',\n",
              " 'dasovich-j',\n",
              " 'presto-k',\n",
              " 'bailey-s',\n",
              " 'farmer-d',\n",
              " 'jones-t',\n",
              " 'forney-j',\n",
              " 'haedicke-m',\n",
              " 'schwieger-j',\n",
              " 'tycholiz-b',\n",
              " 'swerzbin-m',\n",
              " 'mann-k',\n",
              " 'cuilla-m',\n",
              " 'williams-j',\n",
              " 'hain-m',\n",
              " 'harris-s',\n",
              " 'platter-p',\n",
              " 'baughman-d',\n",
              " 'keiser-k',\n",
              " 'wolfe-j',\n",
              " 'lewis-a',\n",
              " 'hayslett-r',\n",
              " 'phanis-s',\n",
              " 'grigsby-m',\n",
              " 'stokley-c',\n",
              " 'shankman-j',\n",
              " 'scott-s',\n",
              " 'griffith-j',\n",
              " 'whalley-g',\n",
              " 'dean-c',\n",
              " 'derrick-j',\n",
              " 'martin-t',\n",
              " 'ring-r',\n",
              " 'keavey-p',\n",
              " 'reitmeyer-j',\n",
              " 'schoolcraft-d',\n",
              " 'mccarty-d',\n",
              " 'rodrique-r',\n",
              " 'sanchez-m',\n",
              " 'guzman-m',\n",
              " 'symes-k',\n",
              " 'giron-d',\n",
              " 'scholtes-d',\n",
              " 'dorland-c',\n",
              " 'whitt-m']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amoIXGpJZF6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import re\n",
        "import sys\n",
        "\n",
        "sys.path.append( \"../tools/\" )\n",
        "from parse_out_email_text import parseOutText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh4D-SIpZMRz",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "    Starter code to process the emails from Sara and Chris to extract\n",
        "    the features and get the documents ready for classification.\n",
        "    \n",
        "    The list of all the emails from Sara are in the from_sara list\n",
        "    likewise for emails from Chris (from_chris)\n",
        "    \n",
        "    The actual documents are in the Enron email dataset, which\n",
        "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
        "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
        "    The data is stored in lists and packed away in pickle files at the end.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc_9BzQpZOPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from_sara  = open(\"from_sara.txt\", \"r\")\n",
        "from_chris = open(\"from_chris.txt\", \"r\")\n",
        "\n",
        "from_data = []\n",
        "word_data = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY8R1m2KZhOY",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "### temp_counter is a way to speed up the development--there are\n",
        "### thousands of emails from Sara and Chris, so running over all of them\n",
        "### can take a long time\n",
        "### temp_counter helps you only look at the first 200 emails in the list so you\n",
        "### can iterate your modifications quicker\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrG8jM7NZs0T",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqEtbf1jZsLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "29849899-5295-46a0-8d18-98cd7644224d"
      },
      "source": [
        "import re\n",
        "\n",
        "temp_counter = 0\n",
        "\n",
        "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
        "    print name\n",
        "    for path in from_person:\n",
        "        ### only look at first 200 emails when developing\n",
        "        ### once everything is working, remove this line to run over full dataset\n",
        "        temp_counter += 1\n",
        "        if temp_counter < 3:\n",
        "            path = os.path.join('..', path[:-1])\n",
        "            print path\n",
        "            email = open(path, \"r\")\n",
        "            \n",
        "            ### use parseOutText to extract the text from the opened email\n",
        "            email_text = parseOutText(email)\n",
        "            print email_text\n",
        "            ### use str.replace() to remove any instances of the words\n",
        "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
        "            pattern = '|'.join([\"sara\", \"shackleton\", \"chris\", \"germani\"])\n",
        "            print pattern                   \n",
        "            #pruner = re.compile(pattern)\n",
        "            email_text = re.sub(pattern, '', email_text)\n",
        "            print email_text\n",
        "            #email_text.replace()\n",
        "            ### append the text to word_data\n",
        "            word_data.append(email_text)\n",
        "\n",
        "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
        "            if name is 'sara':\n",
        "              from_data.append(0)\n",
        "            elif name is 'chris':\n",
        "              from_data.append(1)\n",
        "            else:\n",
        "              from_data.append(2)\n",
        "\n",
        "            email.close()\n",
        "            \n",
        "        else:\n",
        "          break"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sara\n",
            "../maildir/bailey-s/deleted_items/101.\n",
            " sbaile2 NonPrivilegedpst\r\n",
            "\r\n",
            "Susan\n",
            "\n",
            "Please send the foregoing list to Richard  Thanks\n",
            "\n",
            "Sara Shackleton\n",
            "Enron Wholesale Services\n",
            "1400 Smith Street EB3801a\n",
            "Houston TX  77002\n",
            "Ph  713 8535620\n",
            "Fax 713 6463490\n",
            "sara|shackleton|chris|germani\n",
            " sbaile2 NonPrivilegedpst\r\n",
            "\r\n",
            "Susan\n",
            "\n",
            "Please send the foregoing list to Richard  Thanks\n",
            "\n",
            "Sara Shackleton\n",
            "Enron Wholesale Services\n",
            "1400 Smith Street EB3801a\n",
            "Houston TX  77002\n",
            "Ph  713 8535620\n",
            "Fax 713 6463490\n",
            "../maildir/bailey-s/deleted_items/106.\n",
            " sbaile2 NonPrivilegedpst\r\n",
            "\r\n",
            "1\tTXU Energy Trading Company\t\n",
            "\n",
            "2\tBP Capital Energy Fund LP  may be subject to mutual termination\n",
            "\n",
            "2\tNoble Gas Marketing Inc\n",
            "\n",
            "3\tPuget Sound Energy Inc\n",
            "\n",
            "4\tVirginia Power Energy Marketing Inc\n",
            "\n",
            "5\tT Boone Pickens  may be subject to mutual termination \n",
            "\n",
            "5\tNeumin Production Co\n",
            "\n",
            "6\tSodra Skogsagarna Ek For  probably an ECTRIC counterparty\n",
            "\n",
            "6\tTexaco Natural Gas Inc  may be booked incorrectly for Texaco Inc financial trades\n",
            "\n",
            "7\tACE Capital Re Overseas Ltd\n",
            "\n",
            "8\tNevada Power Company\n",
            "\n",
            "9\tPrior Energy Corporation\n",
            "\n",
            "10\tSelect Energy Inc\t\n",
            "\n",
            " Original Message\n",
            "From \tTweed Sheila  \n",
            "Sent\tThursday January 31 2002 310 PM\n",
            "To\tShackleton Sara\n",
            "Subject\t\n",
            "\n",
            "Please send me the names of the 10 counterparties that we are evaluating  Thanks\n",
            "sara|shackleton|chris|germani\n",
            " sbaile2 NonPrivilegedpst\r\n",
            "\r\n",
            "1\tTXU Energy Trading Company\t\n",
            "\n",
            "2\tBP Capital Energy Fund LP  may be subject to mutual termination\n",
            "\n",
            "2\tNoble Gas Marketing Inc\n",
            "\n",
            "3\tPuget Sound Energy Inc\n",
            "\n",
            "4\tVirginia Power Energy Marketing Inc\n",
            "\n",
            "5\tT Boone Pickens  may be subject to mutual termination \n",
            "\n",
            "5\tNeumin Production Co\n",
            "\n",
            "6\tSodra Skogsagarna Ek For  probably an ECTRIC counterparty\n",
            "\n",
            "6\tTexaco Natural Gas Inc  may be booked incorrectly for Texaco Inc financial trades\n",
            "\n",
            "7\tACE Capital Re Overseas Ltd\n",
            "\n",
            "8\tNevada Power Company\n",
            "\n",
            "9\tPrior Energy Corporation\n",
            "\n",
            "10\tSelect Energy Inc\t\n",
            "\n",
            " Original Message\n",
            "From \tTweed Sheila  \n",
            "Sent\tThursday January 31 2002 310 PM\n",
            "To\tShackleton Sara\n",
            "Subject\t\n",
            "\n",
            "Please send me the names of the 10 counterparties that we are evaluating  Thanks\n",
            "chris\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YX7AuOXuZ6F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "79bcabb6-c2e8-4496-e95c-7689127a6dad"
      },
      "source": [
        "word_data"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCLJ2R5PaFKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f535183f-79d9-414c-ff9a-6773fbf8d9e9"
      },
      "source": [
        "print \"emails processed\"\n",
        "from_sara.close()\n",
        "from_chris.close()\n",
        "\n",
        "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
        "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emails processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHgZtBmmaJFT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "### in Part 4, do TfIdf vectorization here\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayPbC4f1aKZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}